pipeline:
  - name: read_step
    op_key: read
    params:
      input_file: resources/input_examples/json_demo.json # input file path, support json, jsonl, txt, csv, pdf. See resources/input_examples for examples

  - name: chunk_step
    op_key: chunk
    deps: [read_step] # chunk_step depends on read_step
    params:
      chunk_size: 1024 # chunk size for text splitting
      chunk_overlap: 100 # chunk overlap for text splitting

  - name: build_kg_step
    op_key: build_kg
    deps: [chunk_step] # build_kg depends on chunk_step

  - name: partition_step
    op_key: partition
    deps: [build_kg] # partition_step depends on build_kg
    params:
      method: dfs # partition method, support: dfs, bfs, ece, leiden
      method_params:
        max_units_per_community: 1 # atomic partition, one node or edge per community

  - name: generate_step
    op_key: generate
    deps: [partition_step] # generate_step depends on partition_step
    params:
      method: atomic # atomic, aggregated, multi_hop, cot, vqa
      data_format: Alpaca # Alpaca, Sharegpt, ChatML
