pipeline:
  - name: read_step
    op_key: read
    params:
      input_file: resources/input_examples/csv_demo.csv # input file path, support json, jsonl, txt, pdf. See resources/input_examples for examples

  - name: chunk_step
    op_key: chunk
    deps: [read_step] # chunk_step depends on read_step
    params:
      chunk_size: 1024 # chunk size for text splitting
      chunk_overlap: 100 # chunk overlap for text splitting

  - name: build_kg_step
    op_key: build_kg
    deps: [chunk_step] # build_kg_step depends on chunk_step

  - name: partition_step
    op_key: partition
    deps: [build_kg_step] # partition_step depends on build_kg_step
    params:
      method: ece # ece is a custom partition method based on comprehension loss
      method_params:
        max_units_per_community: 3 # max nodes and edges per community, for multi-hop, we recommend setting it to 3
        min_units_per_community: 3 # min nodes and edges per community, for multi-hop, we recommend setting it to 3
        max_tokens_per_community: 10240 # max tokens per community
        unit_sampling: random # unit sampling strategy, support: random, max_loss, min_loss

  - name: generate_step
    op_key: generate
    deps: [partition_step] # generate_step depends on partition_step
    params:
      method: multi_hop # atomic, aggregated, multi_hop, cot, vqa
      data_format: ChatML # Alpaca, Sharegpt, ChatML
